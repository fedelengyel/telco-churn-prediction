# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F5KOZlE5UOx9c9uLS1xVwPr6L7xLxhEV
"""

import kagglehub

# =======
# 1. DATA
# =======
path = kagglehub.dataset_download("blastchar/telco-customer-churn")
print("Path to dataset files:", path)

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    classification_report,
    roc_auc_score,
    confusion_matrix,
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    RocCurveDisplay
)

df = pd.read_csv(path + "/WA_Fn-UseC_-Telco-Customer-Churn.csv")
data = df.copy()

# ======
# 2. EDA
# ======
data.info()
data.head()
data.describe()
data["Churn"].value_counts()

# ================
# 3. DATA CLEANING
# ================
data.drop(columns=["customerID", "gender"], inplace=True)

data["TotalCharges"] = pd.to_numeric(data["TotalCharges"], errors="coerce")
data.dropna(subset=["TotalCharges"], inplace=True)

# ========================
# 4. VARIABLE CODIFICATION
# ========================
# Binary Yes/No ---> 0/1
binary_cols = ['Partner', 'Dependents', 'PhoneService', 'PaperlessBilling', 'Churn']

for col in binary_cols:
    data[col] = data[col].map({'Yes': 1, 'No': 0})

# One-hot encoding
cols_onehot = [
    'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup',
    'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies',
    'Contract', 'PaymentMethod'
]

data = pd.get_dummies(data, columns=cols_onehot, drop_first=True)

# ===================
# 5. TRAIN-TEST SPLIT
# ===================
X = data.drop(columns=["Churn"])
y = data["Churn"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

# ==================
# 6. STANDARDIZATION
# ==================
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ==============
# 7. LOGIT MODEL
# ==============
logit = LogisticRegression(max_iter=2000, class_weight='balanced')
logit.fit(X_train_scaled, y_train)

y_proba_logit = logit.predict_proba(X_test_scaled)[:, 1]
y_pred_logit_default = logit.predict(X_test_scaled)

# =======================
# 8. LOGIT WITH THRESHOLD
# =======================
threshold = 0.35

y_proba_logit_035 = y_proba_logit
y_pred_logit_035 = (y_proba_logit_035 >= threshold).astype(int)

# ================
# 9. RANDOM FOREST
# ================
rf = RandomForestClassifier(
    n_estimators=400,
    max_depth=15,
    min_samples_leaf=2,
    max_features='sqrt',
    random_state=42,
    class_weight='balanced'
)

rf.fit(X_train, y_train)

y_proba_rf = rf.predict_proba(X_test)[:, 1]
y_pred_rf_default = rf.predict(X_test)

threshold = 0.35
y_pred_rf_035 = (y_proba_rf >= threshold).astype(int)

# ===================================
# 10. METRICS (LOGIT vs RF, thr=0.35)
# ===================================
cm_logit = confusion_matrix(y_test, y_pred_logit_035)
cm_rf = confusion_matrix(y_test, y_pred_rf_035)

metrics = {
    "Accuracy": [
        accuracy_score(y_test, y_pred_logit_035),
        accuracy_score(y_test, y_pred_rf_035)
    ],
    "Precision": [
        precision_score(y_test, y_pred_logit_035),
        precision_score(y_test, y_pred_rf_035)
    ],
    "Recall": [
        recall_score(y_test, y_pred_logit_035),
        recall_score(y_test, y_pred_rf_035)
    ],
    "F1 Score": [
        f1_score(y_test, y_pred_logit_035),
        f1_score(y_test, y_pred_rf_035)
    ],
    "ROC-AUC": [
        roc_auc_score(y_test, y_proba_logit_035),
        roc_auc_score(y_test, y_proba_rf)
    ]
}

metrics_df = pd.DataFrame(metrics, index=["Logit (thr=0.35)", "RF (thr=0.35)"])
print(metrics_df)

# ===========
# 11. GRAPHICS
# ===========
# Confusion Matrix - Logistic Regression
plt.figure(figsize=(5, 4))
sns.heatmap(cm_logit, annot=True, fmt="d", cmap="Blues")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix - Logistic Regression (thr = 0.35)")
plt.show()

# Confusion Matrix - Random Forest
plt.figure(figsize=(5, 4))
sns.heatmap(cm_rf, annot=True, fmt="d", cmap="Greens")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix - Random Forest (thr = 0.35)")
plt.show()

# ROC Curve - Logistic Regression
plt.figure(figsize=(6, 4))
RocCurveDisplay.from_predictions(y_test, y_proba_logit_035)
plt.title("ROC Curve - Logistic Regression")
plt.show()

# ROC Curve - Random Forest
plt.figure(figsize=(6, 4))
RocCurveDisplay.from_predictions(y_test, y_proba_rf)
plt.title("ROC Curve - Random Forest")
plt.show()

# Distribution of probabilities - Logistic Regression
plt.figure(figsize=(6, 4))
plt.hist(y_proba_logit_035, bins=30, edgecolor="black")
plt.title("Distribution of Predicted Churn Probabilities - Logit")
plt.xlabel("Predicted probability of churn")
plt.ylabel("Count")
plt.show()

# ===========================
# 12. FEATURE IMPORTANCE (RF)
# ===========================

# Extraer importancias
importances = rf.feature_importances_
feature_names = X.columns
feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False).head(10)

# Gráfico de barras
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=feature_importance_df, palette='viridis')
plt.title('Top 10 Variables más influyentes en el Churn (Random Forest)')
plt.xlabel('Importancia Relativa')
plt.ylabel('Variable')
plt.show()